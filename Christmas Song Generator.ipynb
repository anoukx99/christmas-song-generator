{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from itertools import chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(filename):\n",
    "    \"\"\"\n",
    "    Makes a list of all the paths that fit the search requirement\n",
    "    \n",
    "    :param filename: A regular expression that defines the search requirement for the filenames\n",
    "    :return  Returns a list of all the pathnames\n",
    "    \"\"\"\n",
    "    # place the movies folder in the same directory as this notebook\n",
    "    current_directory = os.getcwd()\n",
    "\n",
    "    # glob.glob() is a pattern-matching path finder, it searches for the reviews in the movies folder based on a Regular Expression\n",
    "    paths = glob.glob(current_directory + '/songtexts/' + filename)\n",
    "    \n",
    "    if len(paths) == 0:\n",
    "        print('Your file list is empty. The code looks for the folder '+current_directory+'/songtexts, but could not find it.')\n",
    "    else: \n",
    "        print(\"You loaded: \", len(paths), \"files\")\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(pathset):\n",
    "    \"\"\"\n",
    "    Loads the data into a dataframe\n",
    "    \n",
    "    :param pathset:  A list of paths\n",
    "    :return  A dataframe with three columns: Path, Review (Text) and Label\n",
    "    \"\"\"\n",
    "    # Files are named by sentiment (P for positive, N for negative)\n",
    "    pattern = re.compile('song[0-9]*.txt')\n",
    "    songtext = []\n",
    "    df = pd.DataFrame(columns = ['Path', 'Songtext'])\n",
    "    for path in pathset:\n",
    "        if re.search(pattern, path):\n",
    "            text = open(path, \"r\").read()\n",
    "            songtext.append(text)\n",
    "        else:\n",
    "            text = open(path, \"r\").read()\n",
    "            reviews.append(text)\n",
    "    df['Path'] = pathset\n",
    "    df['Songtext'] = songtext\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You loaded:  100 files\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Songtext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c:\\Users\\anouk\\Documents\\Git-repositories\\chri...</td>\n",
       "      <td>Chestnuts roasting on an open fire\\nJack Frost...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c:\\Users\\anouk\\Documents\\Git-repositories\\chri...</td>\n",
       "      <td>It was Christmas Eve babe\\nIn the drunk tank\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c:\\Users\\anouk\\Documents\\Git-repositories\\chri...</td>\n",
       "      <td>\\nDeck the halls with boughs of holly\\nFa-la-l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c:\\Users\\anouk\\Documents\\Git-repositories\\chri...</td>\n",
       "      <td>Christmas time is here\\nHappiness and cheer\\nF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c:\\Users\\anouk\\Documents\\Git-repositories\\chri...</td>\n",
       "      <td>Santa tell me if you're really there\\nDon't ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Path  \\\n",
       "0  c:\\Users\\anouk\\Documents\\Git-repositories\\chri...   \n",
       "1  c:\\Users\\anouk\\Documents\\Git-repositories\\chri...   \n",
       "2  c:\\Users\\anouk\\Documents\\Git-repositories\\chri...   \n",
       "3  c:\\Users\\anouk\\Documents\\Git-repositories\\chri...   \n",
       "4  c:\\Users\\anouk\\Documents\\Git-repositories\\chri...   \n",
       "\n",
       "                                            Songtext  \n",
       "0  Chestnuts roasting on an open fire\\nJack Frost...  \n",
       "1  It was Christmas Eve babe\\nIn the drunk tank\\n...  \n",
       "2  \\nDeck the halls with boughs of holly\\nFa-la-l...  \n",
       "3  Christmas time is here\\nHappiness and cheer\\nF...  \n",
       "4  Santa tell me if you're really there\\nDon't ma...  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = get_path('song[0-9]*.txt')\n",
    "data = load_data(paths)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def nltk_tokenizer(text):\n",
    "\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    print(tokenized_text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def tokenize_songs(songs):\n",
    "\n",
    "    tokenized_songs = []\n",
    "\n",
    "    for x in songs:\n",
    "      \n",
    "        \n",
    "        translator = x.maketrans('', '', string.punctuation) \n",
    "        data = x.translate(translator)                       #remove the punctuation \n",
    "        data = data.lower()                                  #making the data all lower case\n",
    "        print(data);\n",
    "        tokenized = word_tokenize(x)\n",
    "      \n",
    "        tokenized_songs.append(tokenized)    \n",
    "    \n",
    "    return tokenized_songs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chestnuts roasting on an open fire\n",
      "jack frost nipping at your nose\n",
      "yuletide carols being sung by a choir\n",
      "and folks dressed up like eskimos\n",
      "\n",
      "everybody knows a turkey and some mistletoe\n",
      "help to make the season bright\n",
      "tiny tots with their eyes all aglow\n",
      "will find it hard to sleep tonight\n",
      "they know that santas on his way\n",
      "hes loaded lots of toys and goodies on his sleigh\n",
      "and every mothers child is gonna spy\n",
      "to see if reindeer really know how to fly\n",
      "\n",
      "and so im offering this simple phrase\n",
      "to kids from one to ninetytwo\n",
      "although its been said many times many ways\n",
      "merry christmas to you\n",
      "\n",
      "and so im offering this simple phrase\n",
      "to kids from one to ninetytwo\n",
      "although its been said many times many ways\n",
      "merry christmas to you\n",
      "\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\anouk/nltk_data'\n    - 'c:\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\anouk\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[110], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mToks\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_songs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSongtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m data\u001b[38;5;241m.\u001b[39mhead()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m####Calculating the average sentence length\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[109], line 14\u001b[0m, in \u001b[0;36mtokenize_songs\u001b[1;34m(songs)\u001b[0m\n\u001b[0;32m     12\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mlower()                                  \u001b[38;5;66;03m#making the data all lower case\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(data);\n\u001b[1;32m---> 14\u001b[0m     tokenized \u001b[38;5;241m=\u001b[39m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     tokenized_songs\u001b[38;5;241m.\u001b[39mappend(tokenized)    \n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_songs\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\anouk/nltk_data'\n    - 'c:\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\anouk\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "data['Toks'] = tokenize_songs(data['Songtext'])\n",
    "data.head()\n",
    "\n",
    "####Calculating the average sentence length\n",
    "\n",
    "lengths = []\n",
    "words = []\n",
    "\n",
    "for x in data['Songtext']:\n",
    "    sentence = x.split(\"\\n\")\n",
    "   \n",
    "    count_words = 0\n",
    "    \n",
    "    for y in sentence:\n",
    "        \n",
    "        length = len(y.split())\n",
    "        lengths.append(length)\n",
    "        #print(length)\n",
    "        count_words += length\n",
    "        \n",
    "    words.append(count_words)\n",
    "    \n",
    " \n",
    "print(np.mean(words))      ##words per song\n",
    "print(np.mean(lengths))    #words per sentence\n",
    "print(np.sum(lengths))     #total words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(unigram_frequ):\n",
    "    \n",
    "\n",
    "    \n",
    "    vocabulary = {words:freq for (words,freq) in unigram_frequ.items() if freq > 24}\n",
    "  #  vocabulary = sorted(vocabulary.items(), key=lambda x: x[1], reverse = True)\n",
    "    \n",
    "\n",
    "        \n",
    "    return vocabulary\n",
    "\n",
    "def get_frequencies(ngrams):\n",
    "\n",
    "    ngram_frequencies = Counter(ngrams)\n",
    "    ngram_frequencies = dict(ngram_frequencies) \n",
    "    \n",
    "\n",
    "\n",
    "    return ngram_frequencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = list(chain.from_iterable(data['Toks']))\n",
    "ngrams = list(ngrams(unigrams, 6))\n",
    "\n",
    "ngram_joined  = []\n",
    "\n",
    "for x in ngrams:\n",
    "    y = \" \".join(x)\n",
    "\n",
    "    ngram_joined.append(y)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking for the most frequent words in our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1166),\n",
       " ('and', 621),\n",
       " ('a', 544),\n",
       " ('christmas', 530),\n",
       " ('to', 491),\n",
       " ('you', 486),\n",
       " ('i', 368),\n",
       " ('on', 328),\n",
       " ('in', 308),\n",
       " ('of', 296),\n",
       " ('is', 253),\n",
       " ('all', 232),\n",
       " ('me', 210),\n",
       " ('oh', 197),\n",
       " ('for', 193),\n",
       " ('be', 193),\n",
       " ('this', 192),\n",
       " ('it', 187),\n",
       " ('with', 181),\n",
       " ('my', 171),\n",
       " ('santa', 167),\n",
       " ('its', 163),\n",
       " ('that', 156),\n",
       " ('merry', 146),\n",
       " ('so', 139),\n",
       " ('we', 131),\n",
       " ('day', 123),\n",
       " ('love', 120),\n",
       " ('but', 119),\n",
       " ('come', 114),\n",
       " ('time', 110),\n",
       " ('your', 109),\n",
       " ('he', 108),\n",
       " ('year', 103),\n",
       " ('claus', 90),\n",
       " ('here', 85),\n",
       " ('know', 82),\n",
       " ('have', 81),\n",
       " ('if', 79),\n",
       " ('when', 79),\n",
       " ('bells', 78),\n",
       " ('are', 78),\n",
       " ('na', 77),\n",
       " ('let', 77),\n",
       " ('just', 77),\n",
       " ('will', 75),\n",
       " ('what', 74),\n",
       " ('night', 73),\n",
       " ('up', 70),\n",
       " ('yeah', 70),\n",
       " ('his', 69),\n",
       " ('im', 69),\n",
       " ('no', 69),\n",
       " ('out', 69),\n",
       " ('snow', 69),\n",
       " ('good', 68),\n",
       " ('dont', 67),\n",
       " ('down', 67),\n",
       " ('like', 66),\n",
       " ('one', 66),\n",
       " ('as', 66),\n",
       " ('see', 64),\n",
       " ('was', 64),\n",
       " ('him', 63),\n",
       " ('go', 63),\n",
       " ('run', 63),\n",
       " ('pum', 63),\n",
       " ('wish', 62),\n",
       " ('baby', 61),\n",
       " ('can', 61),\n",
       " ('were', 59),\n",
       " ('us', 58),\n",
       " ('they', 57),\n",
       " ('new', 57),\n",
       " ('ho', 56),\n",
       " ('make', 55),\n",
       " ('from', 55),\n",
       " ('now', 55),\n",
       " ('jingle', 55),\n",
       " ('well', 54),\n",
       " ('sing', 54),\n",
       " ('say', 54),\n",
       " ('little', 54),\n",
       " ('got', 53),\n",
       " ('thats', 53),\n",
       " ('peace', 53),\n",
       " ('ill', 52),\n",
       " ('way', 51),\n",
       " ('home', 51),\n",
       " ('hear', 51),\n",
       " ('our', 49),\n",
       " ('there', 48),\n",
       " ('world', 48),\n",
       " ('sleigh', 47),\n",
       " ('gon', 47),\n",
       " ('want', 47),\n",
       " ('tree', 47),\n",
       " ('not', 46),\n",
       " ('comes', 45),\n",
       " ('been', 44),\n",
       " ('singing', 44),\n",
       " ('could', 44),\n",
       " ('angels', 44),\n",
       " ('earth', 44),\n",
       " ('them', 43),\n",
       " ('around', 42),\n",
       " ('cause', 42),\n",
       " ('king', 42),\n",
       " ('ive', 41),\n",
       " ('born', 41),\n",
       " ('by', 40),\n",
       " ('children', 40),\n",
       " ('do', 40),\n",
       " ('at', 39),\n",
       " ('happy', 39),\n",
       " ('again', 39),\n",
       " ('mine', 39),\n",
       " ('santas', 38),\n",
       " ('rudolph', 38),\n",
       " ('o', 38),\n",
       " ('bright', 37),\n",
       " ('tonight', 37),\n",
       " ('then', 37),\n",
       " ('right', 37),\n",
       " ('really', 36),\n",
       " ('away', 36),\n",
       " ('through', 36),\n",
       " ('christ', 36),\n",
       " ('gave', 36),\n",
       " ('two', 36),\n",
       " ('reindeer', 35),\n",
       " ('song', 35),\n",
       " ('star', 35),\n",
       " ('had', 35),\n",
       " ('where', 35),\n",
       " ('mistletoe', 34),\n",
       " ('need', 34),\n",
       " ('town', 34),\n",
       " ('over', 34),\n",
       " ('get', 33),\n",
       " ('who', 33),\n",
       " ('lord', 33),\n",
       " ('said', 32),\n",
       " ('old', 32),\n",
       " ('god', 32),\n",
       " ('would', 32),\n",
       " ('very', 32),\n",
       " ('how', 31),\n",
       " ('wont', 31),\n",
       " ('heart', 31),\n",
       " ('theres', 31),\n",
       " ('nick', 31),\n",
       " ('true', 30),\n",
       " ('tell', 30),\n",
       " ('lets', 30),\n",
       " ('coming', 30),\n",
       " ('their', 29),\n",
       " ('hes', 28),\n",
       " ('wan', 28),\n",
       " ('light', 28),\n",
       " ('okay', 28),\n",
       " ('why', 27),\n",
       " ('sweet', 27),\n",
       " ('child', 26),\n",
       " ('more', 26),\n",
       " ('fun', 26),\n",
       " ('three', 26),\n",
       " ('next', 25),\n",
       " ('please', 25),\n",
       " ('dong', 25),\n",
       " ('click', 25)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_frequencies = get_frequencies(unigrams)\n",
    "vocabulary = get_vocabulary(unigram_frequencies)\n",
    "vocabulary\n",
    "sorted(vocabulary.items(), key=lambda x: x[1], reverse = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(ngram_joined)\n",
    "sequences = tokenizer.texts_to_sequences(ngram_joined)\n",
    "                                         \n",
    "voca_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into input and output\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=voca_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 5, 150)            374400    \n",
      "_________________________________________________________________\n",
      "lstm_24 (LSTM)               (None, 5, 150)            180600    \n",
      "_________________________________________________________________\n",
      "lstm_25 (LSTM)               (None, 150)               180600    \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 150)               22650     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 2496)              376896    \n",
      "=================================================================\n",
      "Total params: 1,135,146\n",
      "Trainable params: 1,135,146\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(voca_size, 150, input_length=seq_length))\n",
    "model.add(LSTM(150, return_sequences=True))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(150, activation='relu'))\n",
    "model.add(Dense(voca_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "183/183 [==============================] - 5s 29ms/step - loss: 2.5772 - accuracy: 0.4472\n",
      "Epoch 2/50\n",
      "183/183 [==============================] - 5s 28ms/step - loss: 2.2648 - accuracy: 0.4994\n",
      "Epoch 3/50\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 2.0639 - accuracy: 0.5376\n",
      "Epoch 4/50\n",
      "183/183 [==============================] - 5s 26ms/step - loss: 1.8781 - accuracy: 0.5743\n",
      "Epoch 5/50\n",
      "183/183 [==============================] - 5s 29ms/step - loss: 1.7050 - accuracy: 0.6088\n",
      "Epoch 6/50\n",
      "183/183 [==============================] - 6s 32ms/step - loss: 1.5584 - accuracy: 0.6428\n",
      "Epoch 7/50\n",
      "183/183 [==============================] - 5s 29ms/step - loss: 1.4170 - accuracy: 0.6686\n",
      "Epoch 8/50\n",
      "183/183 [==============================] - 5s 26ms/step - loss: 1.2830 - accuracy: 0.6981\n",
      "Epoch 9/50\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 1.1628 - accuracy: 0.7266\n",
      "Epoch 10/50\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 1.0677 - accuracy: 0.7465\n",
      "Epoch 11/50\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.9596 - accuracy: 0.7703\n",
      "Epoch 12/50\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.8740 - accuracy: 0.7913\n",
      "Epoch 13/50\n",
      "183/183 [==============================] - 5s 28ms/step - loss: 0.7984 - accuracy: 0.8096\n",
      "Epoch 14/50\n",
      "183/183 [==============================] - 5s 28ms/step - loss: 0.7204 - accuracy: 0.8277\n",
      "Epoch 15/50\n",
      "183/183 [==============================] - 5s 27ms/step - loss: 0.6513 - accuracy: 0.8433\n",
      "Epoch 16/50\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.5936 - accuracy: 0.8601\n",
      "Epoch 17/50\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.5357 - accuracy: 0.8717\n",
      "Epoch 18/50\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.4789 - accuracy: 0.8898\n",
      "Epoch 19/50\n",
      "183/183 [==============================] - 5s 27ms/step - loss: 0.4372 - accuracy: 0.8964\n",
      "Epoch 20/50\n",
      "183/183 [==============================] - 6s 31ms/step - loss: 0.3937 - accuracy: 0.9087 0s - l\n",
      "Epoch 21/50\n",
      "183/183 [==============================] - 5s 29ms/step - loss: 0.3605 - accuracy: 0.9154\n",
      "Epoch 22/50\n",
      "183/183 [==============================] - 5s 27ms/step - loss: 0.3310 - accuracy: 0.9241\n",
      "Epoch 23/50\n",
      "183/183 [==============================] - 5s 27ms/step - loss: 0.3079 - accuracy: 0.9295\n",
      "Epoch 24/50\n",
      "183/183 [==============================] - 5s 28ms/step - loss: 0.2877 - accuracy: 0.9330\n",
      "Epoch 25/50\n",
      "183/183 [==============================] - 5s 26ms/step - loss: 0.2725 - accuracy: 0.9366\n",
      "Epoch 26/50\n",
      "183/183 [==============================] - 5s 28ms/step - loss: 0.2540 - accuracy: 0.9398\n",
      "Epoch 27/50\n",
      "183/183 [==============================] - 5s 30ms/step - loss: 0.2347 - accuracy: 0.9431\n",
      "Epoch 28/50\n",
      "183/183 [==============================] - 6s 31ms/step - loss: 0.2182 - accuracy: 0.9473\n",
      "Epoch 29/50\n",
      "183/183 [==============================] - 5s 28ms/step - loss: 0.1980 - accuracy: 0.9512\n",
      "Epoch 30/50\n",
      "183/183 [==============================] - 5s 26ms/step - loss: 0.1943 - accuracy: 0.9511\n",
      "Epoch 31/50\n",
      "183/183 [==============================] - 6s 31ms/step - loss: 0.1977 - accuracy: 0.9516\n",
      "Epoch 32/50\n",
      "183/183 [==============================] - 5s 28ms/step - loss: 0.2034 - accuracy: 0.9482\n",
      "Epoch 33/50\n",
      "183/183 [==============================] - 5s 27ms/step - loss: 0.1969 - accuracy: 0.9497\n",
      "Epoch 34/50\n",
      "183/183 [==============================] - 6s 30ms/step - loss: 0.1938 - accuracy: 0.9496\n",
      "Epoch 35/50\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.1868 - accuracy: 0.9503\n",
      "Epoch 36/50\n",
      "183/183 [==============================] - 5s 30ms/step - loss: 0.1804 - accuracy: 0.9514\n",
      "Epoch 37/50\n",
      "183/183 [==============================] - 6s 34ms/step - loss: 0.1716 - accuracy: 0.9524\n",
      "Epoch 38/50\n",
      "183/183 [==============================] - 6s 30ms/step - loss: 0.1616 - accuracy: 0.9557\n",
      "Epoch 39/50\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.1503 - accuracy: 0.9586\n",
      "Epoch 40/50\n",
      "183/183 [==============================] - 5s 26ms/step - loss: 0.1388 - accuracy: 0.9599\n",
      "Epoch 41/50\n",
      "183/183 [==============================] - 5s 26ms/step - loss: 0.1457 - accuracy: 0.9583\n",
      "Epoch 42/50\n",
      "183/183 [==============================] - 5s 26ms/step - loss: 0.1548 - accuracy: 0.9558\n",
      "Epoch 43/50\n",
      "183/183 [==============================] - 5s 28ms/step - loss: 0.1729 - accuracy: 0.9505\n",
      "Epoch 44/50\n",
      "183/183 [==============================] - 5s 26ms/step - loss: 0.1687 - accuracy: 0.9506\n",
      "Epoch 45/50\n",
      "183/183 [==============================] - 5s 29ms/step - loss: 0.1767 - accuracy: 0.9480\n",
      "Epoch 46/50\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.1609 - accuracy: 0.9530 0s - l\n",
      "Epoch 47/50\n",
      "183/183 [==============================] - 5s 28ms/step - loss: 0.1544 - accuracy: 0.9540\n",
      "Epoch 48/50\n",
      "183/183 [==============================] - 5s 28ms/step - loss: 0.1436 - accuracy: 0.9574\n",
      "Epoch 49/50\n",
      "183/183 [==============================] - 6s 30ms/step - loss: 0.1356 - accuracy: 0.9583\n",
      "Epoch 50/50\n",
      "183/183 [==============================] - 6s 31ms/step - loss: 0.1307 - accuracy: 0.9578\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2323dd05f40>"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.reset_states()\n",
    "model.fit(X, y, batch_size=128, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first word sequence of the song. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    result.append(in_text)\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        \n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "            \n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Christmas grandpa forever oh night the bells in we shine peace war is all now i have about the poor little baby cant he want for christmas the things your eyes made slowly horse whisper and what singing the christmas now the very and just really good to forgot the little\n"
     ]
    }
   ],
   "source": [
    "# generate new text\n",
    "seed_text = \"Christmas\"   \n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)\n",
    "np.savetxt(\"Christmas_song.txt\", [generated], fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
