{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from itertools import chain\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(filename):\n",
    "    \"\"\"\n",
    "    Makes a list of all the paths that fit the search requirement\n",
    "    \n",
    "    :param filename: A regular expression that defines the search requirement for the filenames\n",
    "    :return  Returns a list of all the pathnames\n",
    "    \"\"\"\n",
    "    # place the movies folder in the same directory as this notebook\n",
    "    current_directory = os.getcwd()\n",
    "\n",
    "    # glob.glob() is a pattern-matching path finder, it searches for the reviews in the movies folder based on a Regular Expression\n",
    "    paths = glob.glob(current_directory + '/songtexts/' + filename)\n",
    "    \n",
    "    if len(paths) == 0:\n",
    "        print('Your file list is empty. The code looks for the folder '+current_directory+'/songtexts, but could not find it.')\n",
    "    else: \n",
    "        print(\"You loaded: \", len(paths), \"files\")\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(pathset):\n",
    "    \"\"\"\n",
    "    Loads the data into a dataframe\n",
    "    \n",
    "    :param pathset:  A list of paths\n",
    "    :return  A dataframe with three columns: Path, Review (Text) and Label\n",
    "    \"\"\"\n",
    "    # Files are named by sentiment (P for positive, N for negative)\n",
    "    pattern = re.compile('song[0-9]*.txt')\n",
    "    songtext = []\n",
    "    df = pd.DataFrame(columns = ['Path', 'Songtext'])\n",
    "    for path in pathset:\n",
    "        if re.search(pattern, path):\n",
    "            text = open(path, \"r\").read()\n",
    "            songtext.append(text)\n",
    "        else:\n",
    "            text = open(path, \"r\").read()\n",
    "            reviews.append(text)\n",
    "    df['Path'] = pathset\n",
    "    df['Songtext'] = songtext\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = get_path('song[0-9]*.txt')\n",
    "data = load_data(paths)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def nltk_tokenizer(text):\n",
    "\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    print(tokenized_text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_songs(songs):\n",
    "\n",
    "    tokenized_songs = []\n",
    "\n",
    "    for x in songs:\n",
    "      \n",
    "        \n",
    "        translator = x.maketrans('', '', string.punctuation) \n",
    "        data = x.translate(translator)                       #remove the punctuation \n",
    "        data = data.lower()                                  #making the data all lower case\n",
    "        tokenized = word_tokenize(data)\n",
    "      \n",
    "        tokenized_songs.append(tokenized)    \n",
    "    \n",
    "    return tokenized_songs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Toks'] = tokenize_songs(data['Songtext'])\n",
    "data.head()\n",
    "\n",
    "####Calculating the average sentence length\n",
    "\n",
    "lengths = []\n",
    "words = []\n",
    "\n",
    "for x in data['Songtext']:\n",
    "    sentence = x.split(\"\\n\")\n",
    "   \n",
    "    count_words = 0\n",
    "    \n",
    "    for y in sentence:\n",
    "        \n",
    "        length = len(y.split())\n",
    "        lengths.append(length)\n",
    "        #print(length)\n",
    "        count_words += length\n",
    "        \n",
    "    words.append(count_words)\n",
    "    \n",
    " \n",
    "print(np.mean(words))      ##words per song\n",
    "print(np.mean(lengths))    #words per sentence\n",
    "print(np.sum(lengths))     #total words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(unigram_frequ):\n",
    "    \n",
    "\n",
    "    \n",
    "    vocabulary = {words:freq for (words,freq) in unigram_frequ.items() if freq > 24}\n",
    "  #  vocabulary = sorted(vocabulary.items(), key=lambda x: x[1], reverse = True)\n",
    "    \n",
    "\n",
    "        \n",
    "    return vocabulary\n",
    "\n",
    "def get_frequencies(ngrams):\n",
    "\n",
    "    ngram_frequencies = Counter(ngrams)\n",
    "    ngram_frequencies = dict(ngram_frequencies) \n",
    "    \n",
    "\n",
    "\n",
    "    return ngram_frequencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = list(chain.from_iterable(data['Toks']))\n",
    "ngrams = list(ngrams(unigrams, 6))\n",
    "\n",
    "ngram_joined  = []\n",
    "\n",
    "for x in ngrams:\n",
    "    y = \" \".join(x)\n",
    "\n",
    "    ngram_joined.append(y)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking for the most frequent words in our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_frequencies = get_frequencies(unigrams)\n",
    "vocabulary = get_vocabulary(unigram_frequencies)\n",
    "vocabulary\n",
    "sorted(vocabulary.items(), key=lambda x: x[1], reverse = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(ngram_joined)\n",
    "sequences = tokenizer.texts_to_sequences(ngram_joined)\n",
    "                                         \n",
    "voca_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into input and output\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=voca_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(voca_size, 150, input_length=seq_length))\n",
    "model.add(LSTM(150, return_sequences=True))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(150, activation='relu'))\n",
    "model.add(Dense(voca_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.reset_states()\n",
    "model.fit(X, y, batch_size=128, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first word sequence of the song. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    result.append(in_text)\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        \n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "            \n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate new text\n",
    "seed_text = \"Christmas\"   \n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)\n",
    "np.savetxt(\"Christmas_song.txt\", [generated], fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
